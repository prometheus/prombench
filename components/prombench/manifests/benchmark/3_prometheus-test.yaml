apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-test
  namespace: "{{ .PR_NUMBER }}"
data:
  prometheus.yaml: |
    global:
      scrape_interval: 4s

    scrape_configs:
    - job_name: kubelets
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: node

      relabel_configs:
      - action: keep
        source_labels: [__meta_kubernetes_node_label_cloud_google_com_gke_nodepool]
        regex: prometheus-{{ .PR_NUMBER }}|nodes-{{ .PR_NUMBER }}      
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    # Scrapes the endpoint lists for the Kubernetes API server
    # and node-exporter, which we all consider part of a default setup.
    - job_name: standard-endpoints
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # As for kubelets, certificate validation fails for the API server (node)
        # and we circumvent it for now.
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
            - {{ .PR_NUMBER }}

      relabel_configs:
      - action: keep
        source_labels: [__meta_kubernetes_service_label_monitored]
        regex: true
      - action: replace
        source_labels: [__meta_kubernetes_service_name]
        target_label: job
      - action: replace
        source_labels: [__meta_kubernetes_pod_node_name]
        target_label: nodeName

    # Scrapes the endpoint lists for the kube-dns server. Which we consider
    # part of a default setup.
    - job_name: kube-components
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
            - {{ .PR_NUMBER }}

      relabel_configs:
      - action: replace
        source_labels: [__meta_kubernetes_service_label_k8s_app]
        target_label: job
      - action: keep
        source_labels: [__meta_kubernetes_service_name]
        regex: ".*-prometheus-discovery"
      - action: keep
        source_labels: [__meta_kubernetes_endpoint_port_name]
        regex: "http-metrics.*|https-metrics.*"
      - action: replace
        source_labels: [__meta_kubernetes_endpoint_port_name]
        regex: "https-metrics.*"
        target_label: __scheme__
        replacement: https
      - action: replace
        source_labels: [__meta_kubernetes_endpoint_port_name]
        regex: "https-metrics.*"
        target_label: __scheme__
        replacement: https
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-test-{{ normalise .RELEASE }}
  namespace: "{{ .PR_NUMBER }}"
  labels:
    app: prometheus
    prometheus: test-{{ normalise .RELEASE }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
      prometheus: test-{{ normalise .RELEASE }}
  template:
    metadata:
      namespace: "{{ .PR_NUMBER }}"
      labels:
        app: prometheus
        prometheus: test-{{ normalise .RELEASE }}
    spec:
      serviceAccountName: prometheus
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - prometheus
      securityContext:
        runAsUser: 0
      containers:
      - name: prometheus
        image: quay.io/prometheus/prometheus:{{ .RELEASE }}
        imagePullPolicy: Always
        args: [
          "--config.file=/etc/prometheus/config/prometheus.yaml",
          "--storage.tsdb.path=/data",
          "--storage.tsdb.min-block-duration=15m",
          "--storage.tsdb.max-block-duration=4h"
        ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/prometheus/config
        - name: instance-ssd
          mountPath: /data
      volumes:
      - name: config-volume
        configMap:
          name: prometheus-test
      - name: instance-ssd
        hostPath:
          # /mnt is where GKE keeps it's SSD
          # don't change this if you want Prometheus to take advantage of these local SSDs
          path: /mnt/disks/ssd0
      terminationGracePeriodSeconds: 300
      nodeSelector:
        cloud.google.com/gke-nodepool: prometheus-{{ .PR_NUMBER }}
        isolation: prometheus
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-test-{{ normalise .RELEASE }}
  namespace: "{{ .PR_NUMBER }}"
  labels:
    app: prometheus
    prometheus: test-{{ normalise .RELEASE }}
spec:
  ports:
  #should be 1 port
  - name: prometheus
    port: 9090
  selector:
    app: prometheus
    prometheus: test-{{ normalise .RELEASE }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-test-pr-{{ .PR_NUMBER }}
  namespace: "{{ .PR_NUMBER }}"
  labels:
    app: prometheus
    prometheus: test-pr-{{ .PR_NUMBER }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
      prometheus: test-pr-{{ .PR_NUMBER }}
  template:
    metadata:
      namespace: "{{ .PR_NUMBER }}"
      labels:
        app: prometheus
        prometheus: test-pr-{{ .PR_NUMBER }}
    spec:
      serviceAccountName: prometheus
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - prometheus
      securityContext:
        runAsUser: 0
      containers:
      - name: prometheus
        image: docker.io/sipian/prometheus-builder:v1.0.0
        imagePullPolicy: Always
        args: ["{{ .PR_NUMBER }}"]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/prometheus/config
        - name: instance-ssd
          mountPath: /data
      volumes:
      - name: config-volume
        configMap:
          name: prometheus-test
      - name: instance-ssd
        hostPath:
          # /mnt is where GKE keeps it's SSD
          # don't change this if you want Prometheus to take advantage of these local SSDs
          path: /mnt/disks/ssd0
      terminationGracePeriodSeconds: 300
      nodeSelector:
        cloud.google.com/gke-nodepool: prometheus-{{ .PR_NUMBER }}
        isolation: prometheus
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-test-pr-{{ .PR_NUMBER }}
  namespace: "{{ .PR_NUMBER }}"
  labels:
    app: prometheus
    prometheus: test-pr-{{ .PR_NUMBER }}
spec:
  ports:
  #should be 1 port
  - name: prometheus
    port: 9090
  selector:
    app: prometheus
    prometheus: test-pr-{{ .PR_NUMBER }}
